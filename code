

# ================ 1. Imports & Config =========================
import os
from pathlib import Path
from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import (
    confusion_matrix, ConfusionMatrixDisplay,
    roc_auc_score, roc_curve, auc
)
from sklearn.manifold import TSNE  # optional – remove if unused
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Dense, Dropout, LayerNormalization,
    MultiHeadAttention, Add, Flatten
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import shap

# Reproducibility
np.random.seed(42)

tf.keras.utils.set_random_seed(42)

# Suppress TF logs for readability
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# ================ 2. Paths ====================================
DATA_DIR = Path("./data")
RAW_CSV = DATA_DIR / "IBM.csv"                 # input
BAL_CSV = DATA_DIR / "IBM_balanced.csv"        # output balanced data
SHAP_SAMPLE_SIZE = 50                          # reduce for speed if needed

# Create data directory if it does not exist
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ================ 3. Load & Preprocess ========================
print("\n[INFO] Loading data …")

df = pd.read_csv(RAW_CSV)

# Drop unnecessary columns (ignore if missing)
drop_cols = ["EmployeeCount", "Over18", "StandardHours", "EmployeeNumber"]
df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)

# Encode categoricals → integers
cat_cols = df.select_dtypes(include="object").columns
for col in cat_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

# Separate features/label
X = df.drop("Attrition", axis=1)
y = df["Attrition"].astype(int)

# Min-max normalisation (feature-wise)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# ---------------- Minority subset (Attrition == 1) ------------
X_minority = X_scaled[y == 1]

print(f"Dataset shape: {X.shape}, Minority samples: {X_minority.shape[0]}")

# ================ 4. GAN Definition ===========================
print("[INFO] Building GAN …")

latent_dim = 16
input_dim  = X.shape[1]

def build_generator(latent_dim: int, output_dim: int) -> models.Sequential:
    return models.Sequential([
        layers.Input(shape=(latent_dim,)),
        layers.Dense(128, activation="relu"),
        layers.Dense(256, activation="relu"),
        layers.Dense(output_dim, activation="tanh")
    ], name="generator")

def build_discriminator(input_dim: int) -> models.Sequential:
    return models.Sequential([
        layers.Input(shape=(input_dim,)),
        layers.Dense(256, activation="relu"),
        layers.Dense(128, activation="relu"),
        layers.Dense(1, activation="sigmoid")
    ], name="discriminator")

generator     = build_generator(latent_dim, input_dim)
discriminator = build_discriminator(input_dim)

discriminator.compile(optimizer="adam", loss="binary_crossentropy")

# Combined GAN model (generator + frozen discriminator)
noise_in   = layers.Input(shape=(latent_dim,))
synthetic  = generator(noise_in)
discriminator.trainable = False
validity   = discriminator(synthetic)
combined   = Model(noise_in, validity, name="gan")
combined.compile(optimizer="adam", loss="binary_crossentropy")

# ================ 5. Train GAN ================================
print("[INFO] Training GAN … (this may take a while)")

EPOCHS      = 1000
BATCH_SIZE  = 128
PRINT_EVERY = 100

for epoch in range(1, EPOCHS + 1):
    # ----- Train discriminator -----
    idx = np.random.randint(0, X_minority.shape[0], BATCH_SIZE)
    real_samples = X_minority[idx]

    noise = np.random.normal(0, 1, (BATCH_SIZE, latent_dim))
    fake_samples = generator.predict(noise, verbose=0)

    real_labels = np.ones((BATCH_SIZE, 1))
    fake_labels = np.zeros((BATCH_SIZE, 1))

    d_loss_real = discriminator.train_on_batch(real_samples, real_labels)
    d_loss_fake = discriminator.train_on_batch(fake_samples, fake_labels)

    # ----- Train generator (via combined model) -----
    g_loss = combined.train_on_batch(noise, np.ones((BATCH_SIZE, 1)))

    # Logging
    if epoch % PRINT_EVERY == 0 or epoch == 1:
        d_loss = 0.5 * (d_loss_real + d_loss_fake)
        print(f"Epoch {epoch:4d}/{EPOCHS} | D-loss: {d_loss:.4f} | G-loss: {g_loss:.4f}")

# ================ 6. Generate Synthetic Samples ==============
print("[INFO] Generating synthetic samples …")

N_SYNTH = 996  # choose to fully balance classes (empirically sized)
noise     = np.random.normal(0, 1, (N_SYNTH, latent_dim))
X_syn_min = generator.predict(noise, verbose=0)

syn_df = pd.DataFrame(X_syn_min, columns=X.columns)
syn_df["Attrition"] = 1  # minority label

# Combine with original scaled features (convert X_scaled back to DataFrame)
orig_df = pd.DataFrame(X_scaled, columns=X.columns)
orig_df["Attrition"] = y.values

balanced_df = pd.concat([orig_df, syn_df], ignore_index=True)

# Save for later reuse
balanced_df.to_csv(BAL_CSV, index=False)
print(f"[INFO] Balanced dataset saved → {BAL_CSV.relative_to(Path.cwd())}")

# ================ 7. Train / Test Split =======================
print("[INFO] Preparing train/test sets …")

# Re-load (ensures independence from previous vars)
balanced_df = pd.read_csv(BAL_CSV)

# Stratified split: 300 test samples per class
class_0_df = balanced_df[balanced_df["Attrition"] == 0]
class_1_df = balanced_df[balanced_df["Attrition"] == 1]

test_df = pd.concat([
    class_0_df.sample(n=300, random_state=42),
    class_1_df.sample(n=300, random_state=42)
])
train_df = balanced_df.drop(test_df.index).sample(frac=1, random_state=42).reset_index(drop=True)

test_df  = test_df.sample(frac=1, random_state=42).reset_index(drop=True)

X_train = train_df.drop("Attrition", axis=1).values
y_train = train_df["Attrition"].values

X_test  = test_df.drop("Attrition", axis=1).values
y_test  = test_df["Attrition"].values

# Expand dims for Transformer ([samples, features, 1])
X_train_tf = np.expand_dims(X_train, axis=-1)
X_test_tf  = np.expand_dims(X_test,  axis=-1)

print("Training set class distribution:")
for cls, cnt in Counter(y_train).items():
    print(f"  Class {cls}: {cnt} samples")

print("Test set class distribution:")
for cls, cnt in Counter(y_test).items():
    print(f"  Class {cls}: {cnt} samples")

# ================ 8. Transformer Encoder Block ===============

def transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.1):
    # Multi-head self-attention + residual
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads,
                           dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = Add()([x, inputs])

    # Feed-forward network + residual
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return Add()([x, res])

# ================ 9. Build & Compile Model ====================
print("[INFO] Building Transformer model …")

input_layer = Input(shape=(X_train_tf.shape[1], X_train_tf.shape[2]))
enc = transformer_encoder(input_layer)
enc = Flatten()(enc)
enc = Dense(256, activation="relu")(enc)
enc = Dropout(0.4)(enc)
enc = Dense(64, activation="relu")(enc)
enc = Dropout(0.3)(enc)
output_layer = Dense(1, activation="sigmoid")(enc)

model = Model(inputs=input_layer, outputs=output_layer, name="Transformer_Attrition")
model.compile(optimizer=Adam(1e-4), loss="binary_crossentropy", metrics=["accuracy"])

# ================ 10. Train Model =============================
class MetricLogger(Callback):
    """Custom logger printing accuracy/loss and ROC-AUC each epoch."""
    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        y_pred_train = self.model.predict(X_train_tf, verbose=0)
        auc_train = roc_auc_score(y_train, y_pred_train)
        print(f"Epoch {epoch+1:02d} | "
              f"Train Acc: {logs.get('accuracy'):.4f} | Val Acc: {logs.get('val_accuracy'):.4f} | "
              f"Train Loss: {logs.get('loss'):.4f} | Val Loss: {logs.get('val_loss'):.4f} | "
              f"Train AUC: {auc_train:.4f}")

print("[INFO] Training model …")

epochs       = 50
batch_size   = 32
validation_split = 0.2

history = model.fit(
    X_train_tf, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=validation_split,
    verbose=0,
    callbacks=[MetricLogger()]
)

# ================ 11. Plot Training Curves ====================
print("[INFO] Plotting training curves …")

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history["accuracy"], label="Train")
plt.plot(history.history["val_accuracy"], label="Validation")
plt.title("Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history["loss"], label="Train")
plt.plot(history.history["val_loss"], label="Validation")
plt.title("Loss")
plt.xlabel("Epoch")
plt.ylabel("Binary Cross-Entropy")
plt.legend()

plt.tight_layout()
plt.show()

# ================ 12. Evaluate on Test Set ====================
print("[INFO] Evaluating on test data …")

test_loss, test_acc = model.evaluate(X_test_tf, y_test, verbose=0)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Confusion matrix
print("[INFO] Confusion matrix …")

y_pred_prob = model.predict(X_test_tf)
y_pred      = (y_pred_prob > 0.5).astype(int)

cm = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(cm).plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

# ROC curve
print("[INFO] ROC-AUC curve …")

fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ================ 13. SHAP Explainability =====================
print("[INFO] Computing SHAP values … (may be slow)")

# Choose a representative background with K-means summarisation
X_bg = shap.kmeans(X_test[:SHAP_SAMPLE_SIZE], 10)

# Prediction function expects 3-D input → convert inside wrapper
def predict_fn(input_2d):
    return model.predict(np.expand_dims(input_2d, axis=-1)).flatten()

explainer = shap.KernelExplainer(predict_fn, X_bg)
shap_values = explainer.shap_values(X_test[:SHAP_SAMPLE_SIZE], nsamples=100)

feature_names = X.columns.tolist()

# Summary plot
shap.summary_plot(shap_values, X_test[:SHAP_SAMPLE_SIZE], feature_names=feature_names)

# Bar plot of mean |SHAP| importance
abs_shap = np.abs(shap_values)
mean_shap = np.mean(abs_shap, axis=0)

sorted_idx = np.argsort(mean_shap)[::-1]
plt.figure(figsize=(10, 6))
plt.barh(np.array(feature_names)[sorted_idx], mean_shap[sorted_idx])
plt.xlabel("Mean |SHAP Value|")
plt.title("Feature Importance (SHAP)")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

print("\n[✓] Pipeline complete!")
